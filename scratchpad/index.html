<!DOCTYPE html>
<html>
	<head>
		<meta charset="UTF-8">
		<title></title>
		<meta name="viewport" content="width=device-width, initial-scale=1.0" />
		<link rel="stylesheet" href="https://blog.d4s3.xyz/css/main.css" />
	</head>
	<body class='hack'>
		<nav>
			
			<a href="https:&#x2F;&#x2F;blog.d4s3.xyz/"><span itemprop="name">[ Home ]</span></a>
			
			<a href="https:&#x2F;&#x2F;blog.d4s3.xyz/projects"><span itemprop="name">[ Projects ]</span></a>
			
			<a href="https:&#x2F;&#x2F;blog.d4s3.xyz/write_ups"><span itemprop="name">[ Write Ups ]</span></a>
			
			<a href="https:&#x2F;&#x2F;blog.d4s3.xyz/scratchpad"><span itemprop="name">[ scratchpad ]</span></a>
			
		</nav>
		
			
	
		<span id="continue-reading"></span><h1 id="wget-spider">wget spider</h1>
<p><code>wget --mirror --convert-links --page-requisites --adjust-extension &lt;site-url&gt;</code></p>
<ul>
<li><strong>--mirror:</strong> make the download recursive</li>
<li><strong>--convert-links:</strong> convert links to work offline ex. <code>./</code> instead of <code>/</code></li>
<li><strong>--page-requisites:</strong> make sure to download js and css</li>
<li><strong>--adjust-extension:</strong> append missing extensions (mostly for html)</li>
</ul>
<h3 id="warc">WARC</h3>
<p>If I'm crawling a site for an offline backup I probably need a zim file. Wget can't output a zim file but it can output a warc file which can be converted to a zim file using <a href="https://github.com/openzim/warc2zim">warc2zim</a></p>
<ul>
<li>**--warc-file=**filename: enables warc export, standard output will also happen</li>
</ul>
<h3 id="cross-domain">Cross domain</h3>
<ul>
<li><strong>--span-hosts:</strong> crawl cross domain links</li>
<li><strong>--domains:</strong> make sure you only crawl the domains you want to (don't want to accidentally crawl the whole internet)</li>
</ul>

	
		<span id="continue-reading"></span><h1 id="python-one-liners">Python One-Liners</h1>
<p>Takes a list of urls and formats them in markdown
<code>python -c "import sys; from urllib.parse import urlsplit; print(''.join([f\"[{urlsplit(line.strip()).path.lstrip('/')}]({line.strip()})\n\" for line in sys.stdin]))"</code></p>

	
		<span id="continue-reading"></span><h1 id="netcat-webserver">Netcat Webserver</h1>
<p><code>while true; do echo -e "HTTP/1.1 200 OK\n\n" $(cat index.html) | nc -lp 8080 -q 1; done</code></p>
<ul>
<li><em>"HTTP/1.1 200 OK\n\n"</em> can be put in the html file. Just be sure to use <code>^M^M</code> instead of the literal <code>\n\n</code></li>
</ul>

	
		<span id="continue-reading"></span><h1 id="strip-everything-except-ascii-from-text-files">Strip everything except ascii from Text files</h1>
<p>A really quick way of doing this is to use <code>strings</code>. However this will strip whitespace characters.</p>
<p>If you need whitespace characters intact you can use this:</p>
<p><code>cat &lt;file&gt; | tr -d "[:cntrl:]" | iconv -c -f utf-8 -t ascii -</code></p>

	


		
		</div>
	</body>
</html>
